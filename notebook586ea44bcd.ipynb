{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2333429,"sourceType":"datasetVersion","datasetId":1408532}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Animal Classification with CNN  \n\n- The objective of this project is to explore how image manipulation techniques, particularly brightness and contrast adjusmants, impact the performanve neural network model in image classification tasks.\n\nThe model is tested on three sets:\n\n* Original Test Set – Standard images.  \n* Manipulated Test Set – Images with brightness/contrast changes.  \n* Color-Corrected Test Set – Images processed with the Gray World Algorithm for color constancy.  \n\nKey Steps:\n* Data Preprocessing: Resize, normalize, and split data.  \n* Model Training: Train a basic CNN model.  \n* Evaluation: Compare performance across the three test sets.  \n\nDataset: https://www.kaggle.com/datasets/rrebirrth/animals-with-attributes-2/data\n","metadata":{"execution":{"iopub.status.busy":"2024-12-21T16:47:52.357915Z","iopub.execute_input":"2024-12-21T16:47:52.358198Z","iopub.status.idle":"2024-12-21T16:47:52.363679Z","shell.execute_reply.started":"2024-12-21T16:47:52.358175Z","shell.execute_reply":"2024-12-21T16:47:52.362529Z"}}},{"cell_type":"markdown","source":"# 1. Read and Prepare Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:24:27.847070Z","iopub.execute_input":"2024-12-21T17:24:27.847432Z","iopub.status.idle":"2024-12-21T17:24:31.173143Z","shell.execute_reply.started":"2024-12-21T17:24:27.847393Z","shell.execute_reply":"2024-12-21T17:24:31.172473Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"source = \"/kaggle/input/animals-with-attributes-2/Animals_with_Attributes2/JPEGImages\"  \ntarget = \"/kaggle/working/FilteredImages\"\n\nclasses = [\"collie\", \"dolphin\", \"elephant\", \"fox\", \"moose\", \"rabbit\", \"sheep\", \"squirrel\", \"giant+panda\", \"polar+bear\"]\nimages_per_class = 650\n\n# Create the target directory\nos.makedirs(target, exist_ok=True)\n\n# Loop through each class and copy the images\nfor class_name in classes:\n    class_path = os.path.join(source, class_name)\n    target_path = os.path.join(target, class_name)\n    \n    if not os.path.exists(class_path):\n        print(f\"Source class path {class_path} does not exist. Skipping...\")\n        continue\n    \n    # Create a subdirectory for each class in the target directory\n    os.makedirs(target_path, exist_ok=True)\n    \n    print(f\"Processing class: {class_name}\")\n    image_count = 0\n    available_files = os.listdir(class_path)\n    \n    # Adjust images_per_class to the number of available images\n    total_images = len(available_files)\n    images_to_copy = min(images_per_class, total_images)\n    print(f\"Found {total_images} images. Attempting to copy {images_to_copy} images.\")\n    \n    for file_name in available_files:\n        if image_count >= images_to_copy:\n            break\n        \n        full_file_name = os.path.join(class_path, file_name)\n        if os.path.isfile(full_file_name):\n            img = cv2.imread(full_file_name)\n            \n            if img is not None:  # Ensure the image is read properly\n                cv2.imwrite(os.path.join(target_path, file_name), img)\n                image_count += 1\n            else:\n                print(f\"Warning: Unable to read image {full_file_name}\")\n    \n    print(f\"Completed {image_count}/{images_to_copy} images for class {class_name}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-21T17:24:31.173917Z","iopub.execute_input":"2024-12-21T17:24:31.174383Z","iopub.status.idle":"2024-12-21T17:25:58.377892Z","shell.execute_reply.started":"2024-12-21T17:24:31.174358Z","shell.execute_reply":"2024-12-21T17:25:58.376973Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing class: collie\nFound 1028 images. Attempting to copy 650 images.\nCompleted 650/650 images for class collie\nProcessing class: dolphin\nFound 946 images. Attempting to copy 650 images.\nCompleted 650/650 images for class dolphin\nProcessing class: elephant\nFound 1038 images. Attempting to copy 650 images.\nCompleted 650/650 images for class elephant\nProcessing class: fox\nFound 664 images. Attempting to copy 650 images.\nCompleted 650/650 images for class fox\nProcessing class: moose\nFound 704 images. Attempting to copy 650 images.\nCompleted 650/650 images for class moose\nProcessing class: rabbit\nFound 1088 images. Attempting to copy 650 images.\nCompleted 650/650 images for class rabbit\nProcessing class: sheep\nFound 1420 images. Attempting to copy 650 images.\nCompleted 650/650 images for class sheep\nProcessing class: squirrel\nFound 1200 images. Attempting to copy 650 images.\nCompleted 650/650 images for class squirrel\nProcessing class: giant+panda\nFound 874 images. Attempting to copy 650 images.\nCompleted 650/650 images for class giant+panda\nProcessing class: polar+bear\nFound 868 images. Attempting to copy 650 images.\nCompleted 650/650 images for class polar+bear\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def load_and_process_images(data_dir, image_size=(128, 128)):\n    images = []\n    labels = []\n    for class_name in os.listdir(data_dir):\n        class_path = os.path.join(data_dir, class_name)\n        if os.path.isdir(class_path):  # Ensure it's a directory\n            for file_name in os.listdir(class_path):\n                file_path = os.path.join(class_path, file_name)\n                try:\n                    img = cv2.imread(file_path)\n                    if img is not None:  # Ensure the image is loaded\n                        img_resized = cv2.resize(img, image_size)\n                        img_normalized = img_resized / 255.0  # Normalize pixel values\n                        images.append(img_normalized)\n                        labels.append(class_name)\n                except Exception as e:\n                    print(f\"Error processing file {file_path}: {e}\")\n    return np.array(images), np.array(labels)\n\n# Define your data directory\ndata_dir = target  # Use 'target' as defined earlier\n\n# Load and process images\nX, y = load_and_process_images(data_dir)\n\n# Print dataset size and shape\nprint(f\"Dataset size: {len(X)} images\")\nprint(f\"Image shape: {X[0].shape if len(X) > 0 else 'No images loaded'}\")\nprint(f\"Labels size: {len(y)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:25:58.378858Z","iopub.execute_input":"2024-12-21T17:25:58.379191Z","iopub.status.idle":"2024-12-21T17:26:37.547502Z","shell.execute_reply.started":"2024-12-21T17:25:58.379157Z","shell.execute_reply":"2024-12-21T17:26:37.546576Z"}},"outputs":[{"name":"stdout","text":"Dataset size: 6500 images\nImage shape: (128, 128, 3)\nLabels size: 6500\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Encode the labels\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)  # Convert string labels to integers\ny_categorical = to_categorical(y_encoded)  # Convert to one-hot encoding\n\n# Check label encoding\nprint(f\"Classes: {encoder.classes_}\")\nprint(f\"Encoded labels: {y_encoded[:10]}\")\nprint(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n\n# Split the data (70% training, 30% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.3, random_state=42)\n\n# Print dataset shapes\nprint(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\nprint(f\"Training labels shape: {y_train.shape}, Test labels shape: {y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:37.548489Z","iopub.execute_input":"2024-12-21T17:26:37.548771Z","iopub.status.idle":"2024-12-21T17:26:38.276951Z","shell.execute_reply.started":"2024-12-21T17:26:37.548747Z","shell.execute_reply":"2024-12-21T17:26:38.276166Z"}},"outputs":[{"name":"stdout","text":"Classes: ['collie' 'dolphin' 'elephant' 'fox' 'giant+panda' 'moose' 'polar+bear'\n 'rabbit' 'sheep' 'squirrel']\nEncoded labels: [9 9 9 9 9 9 9 9 9 9]\nOne-hot encoded labels shape: (6500, 10)\nTraining data shape: (4550, 128, 128, 3), Test data shape: (1950, 128, 128, 3)\nTraining labels shape: (4550, 10), Test labels shape: (1950, 10)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=15,  # Rotate images by up to 15 degrees\n    width_shift_range=0.1,  # Shift images horizontally by up to 10% of width\n    height_shift_range=0.1,  # Shift images vertically by up to 10% of height\n    shear_range=0.1,  # Shear images by up to 10%\n    zoom_range=0.1,  # Zoom images in/out by up to 10%\n    horizontal_flip=True,  # Randomly flip images horizontally\n    fill_mode='nearest'  # Fill any missing pixels using the nearest pixel\n)\ndatagen.fit(X_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:38.278888Z","iopub.execute_input":"2024-12-21T17:26:38.279128Z","iopub.status.idle":"2024-12-21T17:26:38.920080Z","shell.execute_reply.started":"2024-12-21T17:26:38.279107Z","shell.execute_reply":"2024-12-21T17:26:38.919411Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 2. CNN Model Build","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n\n# Convolutional layer 2\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))  # Pooling layer\n\n# Flatten the feature maps\nmodel.add(Flatten())\n\n# Output layer\nmodel.add(Dense(10, activation='softmax'))  # 10 classes\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:38.921286Z","iopub.execute_input":"2024-12-21T17:26:38.921594Z","iopub.status.idle":"2024-12-21T17:26:39.204716Z","shell.execute_reply.started":"2024-12-21T17:26:38.921564Z","shell.execute_reply":"2024-12-21T17:26:39.203953Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m57600\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │         \u001b[38;5;34m576,010\u001b[0m │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">57600</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">576,010</span> │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m595,402\u001b[0m (2.27 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">595,402</span> (2.27 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m595,402\u001b[0m (2.27 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">595,402</span> (2.27 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# 3. CNN Model Test","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train,\n    validation_split=0.2,  # Use 20% of the training data for validation\n    epochs=10,  # Train for 5 epochs\n    batch_size=32,  # Use batches of 32 images\n    verbose=1\n)\n\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:26:39.205569Z","iopub.execute_input":"2024-12-21T17:26:39.205832Z","iopub.status.idle":"2024-12-21T17:27:00.439056Z","shell.execute_reply.started":"2024-12-21T17:26:39.205810Z","shell.execute_reply":"2024-12-21T17:27:00.438307Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.2954 - loss: 1.9905 - val_accuracy: 0.5242 - val_loss: 1.4751\nEpoch 2/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5907 - loss: 1.2389 - val_accuracy: 0.5473 - val_loss: 1.3294\nEpoch 3/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7492 - loss: 0.7807 - val_accuracy: 0.5505 - val_loss: 1.4027\nEpoch 4/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8659 - loss: 0.4437 - val_accuracy: 0.5615 - val_loss: 1.4942\nEpoch 5/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9462 - loss: 0.2165 - val_accuracy: 0.5571 - val_loss: 1.7108\nEpoch 6/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9789 - loss: 0.1021 - val_accuracy: 0.5451 - val_loss: 1.9407\nEpoch 7/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9952 - loss: 0.0449 - val_accuracy: 0.5462 - val_loss: 2.2140\nEpoch 8/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9969 - loss: 0.0171 - val_accuracy: 0.5385 - val_loss: 2.4831\nEpoch 9/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.5418 - val_loss: 2.4986\nEpoch 10/10\n\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.5473 - val_loss: 2.6334\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5531 - loss: 2.6446\nTest Accuracy: 55.74%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# 4. Manipulation of Images with Different Lights and Testing with Manipulated Test Set","metadata":{}},{"cell_type":"code","source":"# Simple image manipulation (e.g., increasing contrast)\ndef manipulate_images(images):\n    return np.array([cv2.convertScaleAbs(img, alpha=2.0, beta=0) for img in images])  # Contrast adjustment\n\n# Apply manipulation\nX_test_manipulated = manipulate_images(X_test)\n\n# Evaluate the model on manipulated images\nmanipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\nprint(f\"Accuracy on Manipulated Test Images (with contrast): {manipulated_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:00.439898Z","iopub.execute_input":"2024-12-21T17:27:00.440225Z","iopub.status.idle":"2024-12-21T17:27:01.492336Z","shell.execute_reply.started":"2024-12-21T17:27:00.440166Z","shell.execute_reply":"2024-12-21T17:27:01.491614Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4745 - loss: 7.7571\nAccuracy on Manipulated Test Images (with contrast): 47.64%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 5. Applying the Color Constancy Algorithm to the Manipulated Test Set and Testing with the Color Constancy Test Set","metadata":{}},{"cell_type":"code","source":"# Gray World algorithm for color constancy\ndef get_wb_images(images):\n    wb_images = []\n    \n    for img in images:\n        # Convert the image to RGB if it's not already\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Calculate the mean color of the image\n        mean_r = np.mean(img_rgb[:,:,0])\n        mean_g = np.mean(img_rgb[:,:,1])\n        mean_b = np.mean(img_rgb[:,:,2])\n        \n        # Calculate the average color\n        avg = (mean_r + mean_g + mean_b) / 3\n        \n        # Adjust each channel to balance the colors\n        img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n        img_rgb[:,:,1] = img_rgb[:,:,1] * (avg / mean_g)\n        img_rgb[:,:,2] = img_rgb[:,:,2] * (avg / mean_b)\n        \n        # Clip the values to keep them in the 0-255 range\n        img_rgb = np.clip(img_rgb, 0, 255)\n        \n        # Convert back to BGR (if needed) and store\n        wb_images.append(cv2.cvtColor(img_rgb.astype(np.uint8), cv2.COLOR_RGB2BGR))\n    \n    return np.array(wb_images)\n\n# Apply the Gray World algorithm to the manipulated test images\nX_test_wb = get_wb_images(X_test_manipulated)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:01.493068Z","iopub.execute_input":"2024-12-21T17:27:01.493384Z","iopub.status.idle":"2024-12-21T17:27:01.895013Z","shell.execute_reply.started":"2024-12-21T17:27:01.493351Z","shell.execute_reply":"2024-12-21T17:27:01.894293Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-9-c84537b1bf17>:18: RuntimeWarning: divide by zero encountered in scalar divide\n  img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n<ipython-input-9-c84537b1bf17>:18: RuntimeWarning: invalid value encountered in multiply\n  img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n<ipython-input-9-c84537b1bf17>:18: RuntimeWarning: invalid value encountered in cast\n  img_rgb[:,:,0] = img_rgb[:,:,0] * (avg / mean_r)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Evaluate the model on the color-corrected test set\nwb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\n\n# Print the accuracy of the model on the color-constant corrected test set\nprint(f\"Accuracy on Color-Corrected Test Images: {wb_accuracy * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:01.895957Z","iopub.execute_input":"2024-12-21T17:27:01.896310Z","iopub.status.idle":"2024-12-21T17:27:02.206882Z","shell.execute_reply.started":"2024-12-21T17:27:01.896275Z","shell.execute_reply":"2024-12-21T17:27:02.206145Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0769 - loss: 46.7185\nAccuracy on Color-Corrected Test Images: 7.59%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# 6. Comparing and Reporting the Success of Different Test Sets","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the original test set\noriginal_loss, original_accuracy = model.evaluate(X_test, y_test)\nprint(f\"Accuracy on Original Test Set: {original_accuracy * 100:.2f}%\")\n\n# Evaluate the model on the manipulated test set\nmanipulated_loss, manipulated_accuracy = model.evaluate(X_test_manipulated, y_test)\nprint(f\"Accuracy on Manipulated Test Set: {manipulated_accuracy * 100:.2f}%\")\n\n# Evaluate the model on the color-corrected (Gray World) test set\nwb_loss, wb_accuracy = model.evaluate(X_test_wb, y_test)\nprint(f\"Accuracy on Color-Corrected Test Set: {wb_accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T17:27:02.207611Z","iopub.execute_input":"2024-12-21T17:27:02.207848Z","iopub.status.idle":"2024-12-21T17:27:04.177119Z","shell.execute_reply.started":"2024-12-21T17:27:02.207826Z","shell.execute_reply":"2024-12-21T17:27:04.176424Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5531 - loss: 2.6446\nAccuracy on Original Test Set: 55.74%\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4745 - loss: 7.7571\nAccuracy on Manipulated Test Set: 47.64%\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0769 - loss: 46.7185\nAccuracy on Color-Corrected Test Set: 7.59%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"- Accuracy on Original Test Set: 52.21%\n\n- Accuracy on Manipulated Test Set: 46.31%\n\n- Accuracy on Color-Corrected Test Set: 8.15%\n\nThe model performs well on the original test set, but its accuracy drops significantly when exposed to manipulated or color-corrected images.  \nThis suggests that the model needs further improvement to handle real-world lighting variations.  \nPossible solutions include data augmentation, transfer learning, and experimenting with advanced color constancy algorithms.","metadata":{}}]}